import os
import sys
import shutil
import time
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_chroma import Chroma


# âœ… Gemini API í‚¤ ì„¤ì •
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()


# âœ… PDF ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()


# âœ… í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì„ë² ë”©
@st.cache_resource
def create_vector_store(_docs, pdf_path):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    persist_directory = "./chroma_db"
    os.makedirs(persist_directory, exist_ok=True)

    st.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )

    # âœ… PDF ìˆ˜ì • ì‹œê°„ ê¸°ë¡
    timestamp_file = os.path.join(persist_directory, "timestamp.txt")
    with open(timestamp_file, "w") as f:
        f.write(str(os.path.getmtime(pdf_path)))

    st.success("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
    return vectorstore


# âœ… ì €ì¥ëœ Chroma DB ë¶ˆëŸ¬ì˜¤ê¸° or ìƒˆë¡œ ìƒì„±
@st.cache_resource
def get_vectorstore(_docs, pdf_path="ì—°ì§„êµ­.pdf"):
    persist_directory = "./chroma_db"
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    timestamp_file = os.path.join(persist_directory, "timestamp.txt")
    pdf_mtime = os.path.getmtime(pdf_path)

    # ê¸°ì¡´ DBê°€ ì¡´ì¬í•˜ê³  PDFê°€ ì•ˆ ë°”ë€ ê²½ìš°
    if os.path.exists(persist_directory) and os.path.exists(timestamp_file):
        with open(timestamp_file, "r") as f:
            saved_time = float(f.read().strip())
        if abs(pdf_mtime - saved_time) < 1:
            st.info("ğŸ“¦ ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
            return Chroma(
                persist_directory=persist_directory,
                embedding_function=embeddings
            )

    # PDFê°€ ë°”ë€ ê²½ìš° â†’ ìƒˆë¡œ ìƒì„±
    st.warning("ğŸ“‘ ìƒˆ PDF íŒŒì¼ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤...")
    return create_vector_store(_docs, pdf_path)


# âœ… RAG ì²´ì¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_components(selected_model):
    file_path = "ì—°ì§„êµ­.pdf"
    pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore(pages, pdf_path=file_path)
    retriever = vectorstore.as_retriever()

    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    If you don't know the answer, just say that you don't know. \
    Keep the answer perfect. please use imogi with the answer.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\
    ì‹ë‹¨ì¶”ì²œì€ ì¶”ì²œí•´ë‹¬ë¼ê³  í• ë•Œ ê¹Œì§€ ì ˆëŒ€ í•˜ì§€ë§ˆ.\
    ê·¸ë¦¬ê³  ìƒë‹´ìì˜ ê¸°ë¶„ì— ë§ì¶° ì˜ ëŒ€ë‹µí•´ì¤˜.\
    

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    llm = ChatGoogleGenerativeAI(
        model=selected_model,
        temperature=0.7,
        convert_system_message_to_human=True
    )

    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain


# âœ… Streamlit UI
st.header("ğŸ½ï¸ MoodBite")
st.caption("ì‚¬ìš©ìì˜ ëŒ€í™”ë¥¼ ë¶„ì„í•´ í˜„ì¬ ê¸°ë¶„ì„ ì§ì‘í•˜ê³ , ê·¸ì— ë§ëŠ” ìŒì‹ì„ ì¶”ì²œí•´ì£¼ëŠ” ìŠ¤ë§ˆíŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤. ğŸ˜Š\
ì¦ê±°ìš´ ê¸°ë¶„ì—ëŠ” ìƒí¼í•œ ë””ì €íŠ¸ë¥¼, ì§€ì¹œ ê¸°ë¶„ì—ëŠ” ë“ ë“ í•œ í•œ ë¼ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤!")

# ğŸ§¹ ë°ì´í„° ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ğŸ§¹ ë²¡í„° ë°ì´í„° ì´ˆê¸°í™”"):
    if os.path.exists("./chroma_db"):
        shutil.rmtree("./chroma_db")
        st.success("âœ… ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰ ì‹œ ìƒˆ PDFë¡œ ê°±ì‹ ë©ë‹ˆë‹¤.")
        st.stop()
    else:
        st.info("ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ì²« ì‹¤í–‰ ì•ˆë‚´
if not os.path.exists("./chroma_db"):
    st.info("ğŸ”„ ì²« ì‹¤í–‰ì…ë‹ˆë‹¤. ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° PDF ì²˜ë¦¬ ì¤‘... (ì•½ 5-7ë¶„ ì†Œìš”)")
    st.info("ğŸ’¡ ì´í›„ ì‹¤í–‰ì—ì„œëŠ” 10-15ì´ˆë§Œ ê±¸ë¦½ë‹ˆë‹¤!")

# ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select Gemini Model",
    ("gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash-exp"),
    index=0,
    help="Gemini 2.5 Flashê°€ ê°€ì¥ ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤"
)

# ì´ˆê¸°í™”
try:
    with st.spinner("ğŸ”§ ì±—ë´‡ ì´ˆê¸°í™” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
        rag_chain = initialize_components(option)
    st.success("âœ… ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

# íˆìŠ¤í† ë¦¬ ê´€ë¦¬
chat_history = StreamlitChatMessageHistory(key="chat_messages")
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì´ˆê¸° ë©”ì‹œì§€
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant",
                                     "content": "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š MoodBiteì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”?"}]

for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# âœ… ì…ë ¥ì°½ + ì‹ë‹¨ ì¶”ì²œ ë²„íŠ¼ ì¶”ê°€
col1, col2 = st.columns([4, 1])
with col1:
    prompt_message = st.chat_input("ì§€ê¸ˆ ê¸°ë¶„ì´ë‚˜ ìƒí™©ì„ ì´ì•¼ê¸°í•´ë³´ì„¸ìš” ğŸ°")
with col2:
    recommend = st.button("ğŸ± ì‹ë‹¨ ì¶”ì²œ")

# ğŸ’¬ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
if prompt_message:
    st.chat_message("human").write(prompt_message)
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config
            )
            answer = response['answer']
            st.write(answer)
            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc in response['context']:
                    st.markdown(doc.metadata['source'], help=doc.page_content)

# ğŸ± ì‹ë‹¨ ì¶”ì²œ ë²„íŠ¼ ë™ì‘
if recommend:
    st.chat_message("human").write("ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ì„œ ì‹ë‹¨ì„ ì¶”ì²œí•´ì¤˜ ğŸ±")
    with st.chat_message("ai"):
        with st.spinner("GPTê°€ ë©”ë‰´ë¥¼ ê³ ë¯¼ ì¤‘ì´ì—ìš”... ğŸ˜‹"):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {
                    "input": (
                        "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì—ê²Œ ì–´ìš¸ë¦¬ëŠ” ì‹ë‹¨ì„ ì¶”ì²œí•´ì¤˜. "
                        "ê¸°ë¶„ê³¼ ìƒí™©ì„ ë°˜ì˜í•´ì„œ ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ë§íˆ¬ë¡œ ì´ì•¼ê¸°í•´ì¤˜. "
                        "ìŒì‹ ì´ë¦„ê³¼ ê°„ë‹¨í•œ ì´ìœ ë„ í•¨ê»˜ ì•Œë ¤ì¤˜."
                    )
                },
                config
            )
            answer = response['answer']
            st.write(answer)
            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc in response['context']:
                    st.markdown(doc.metadata['source'], help=doc.page_content)
