import os
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

# pysqlite3ë¥¼ ì‚¬ìš©í•˜ì—¬ ChromaDB í˜¸í™˜ì„± í™•ë³´
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_chroma import Chroma


# Gemini API í‚¤ ì„¤ì •
try:
    # Streamlit Secretsì—ì„œ API í‚¤ ë¡œë“œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ)
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_and_split_pdf(file_path):
    # PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í˜ì´ì§€ë³„ë¡œ ë¶„í• 
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    persist_directory = "./chroma_db"
    st.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
    # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )
    st.success("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
    return vectorstore

# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db"
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    if os.path.exists(persist_directory):
        # ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ ë¡œë“œ
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    else:
        # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        return create_vector_store(_docs)
    
# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•
@st.cache_resource
def initialize_components(selected_model):
    # íŒŒì¼ ê²½ë¡œ: ì‚¬ìš©ìê°€ ìš”ì²­í•œ 'ì—°ì§„êµ­.pdf' ì‚¬ìš©
    file_path = "ì—°ì§„êµ­.pdf"
    pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is. (í•œêµ­ì–´ ë‹µë³€)"""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # **ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (MoodBite ì—­í• ì— ë§ê²Œ ìˆ˜ì •)**
    qa_system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ì ê¸°ë¶„ì„ ë¶„ì„í•˜ê³  ìŒì‹ ë˜ëŠ” ì¶”ì²œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•˜ê³  ìŠ¤ë§ˆíŠ¸í•œ í‘¸ë“œ íë ˆì´í„°ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê±°ë‚˜ ê¸°ë¶„ì„ í‘œí˜„í•˜ë©´, í˜„ì¬ ëŒ€í™”ì˜ ë§¥ë½(context)ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê¸°ë¶„ì„ ì§ì‘í•˜ê³  ê·¸ ê¸°ë¶„ì— ê°€ì¥ ì í•©í•œ ìŒì‹ì´ë‚˜ ë””ì €íŠ¸ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
    ë‹µë³€ì€ ë°˜ë“œì‹œ ì£¼ì–´ì§„ {context}ë¥¼ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ìŒì‹ ì¢…ë¥˜, ë©”ë‰´ ì„¤ëª…, ê´€ë ¨ ì¥ì†Œ)ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    ë§Œì•½ {context}ì— ì¶”ì²œí•  ë§Œí•œ ì •ë³´ê°€ ì—†ë‹¤ë©´, "ì£„ì†¡í•˜ì§€ë§Œ ì´ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ê¸°ë¶„ì— ë§ëŠ” êµ¬ì²´ì ì¸ ìŒì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê¸°ë¶„ì— ëŒ€í•´ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?" ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ê¸°ë¶„ì— ë§ëŠ” ì´ëª¨í‹°ì½˜ì„ í¬í•¨í•˜ì—¬ ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ì–´íˆ¬ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”.

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    try:
        llm = ChatGoogleGenerativeAI(
            model=selected_model,
            temperature=0.7,
            convert_system_message_to_human=True
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ '{selected_model}' ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.info("ğŸ’¡ 'gemini-2.5-flash' ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
        raise

    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# Streamlit UI
st.header("MoodBite")
# ì—…ë°ì´íŠ¸ëœ ì†Œê°œ ë©”ì‹œì§€
st.markdown("""
**ì‚¬ìš©ìì˜ ëŒ€í™”ë¥¼ ë¶„ì„í•´ í˜„ì¬ ê¸°ë¶„ì„ ì§ì‘í•˜ê³ , ê·¸ì— ë§ëŠ”import os
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

# pysqlite3ë¥¼ ì‚¬ìš©í•˜ì—¬ ChromaDB í˜¸í™˜ì„± í™•ë³´
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_chroma import Chroma


# Gemini API í‚¤ ì„¤ì •
try:
    # Streamlit Secretsì—ì„œ API í‚¤ ë¡œë“œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ)
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_and_split_pdf(file_path):
    # PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í˜ì´ì§€ë³„ë¡œ ë¶„í• 
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    persist_directory = "./chroma_db"
    st.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
    # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )
    st.success("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
    return vectorstore

# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db"
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    if os.path.exists(persist_directory):
        # ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ ë¡œë“œ
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    else:
        # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        return create_vector_store(_docs)
    
# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•
@st.cache_resource
def initialize_components(selected_model):
    # íŒŒì¼ ê²½ë¡œ: ì‚¬ìš©ìê°€ ìš”ì²­í•œ 'ì—°ì§„êµ­.pdf' ì‚¬ìš©
    file_path = "ì—°ì§„êµ­.pdf"
    pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is. (í•œêµ­ì–´ ë‹µë³€)"""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # **ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (MoodBite ì—­í• ì— ë§ê²Œ ìˆ˜ì •)**
    qa_system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ì ê¸°ë¶„ì„ ë¶„ì„í•˜ê³  ìŒì‹ ë˜ëŠ” ì¶”ì²œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•˜ê³  ìŠ¤ë§ˆíŠ¸í•œ í‘¸ë“œ íë ˆì´í„°ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê±°ë‚˜ ê¸°ë¶„ì„ í‘œí˜„í•˜ë©´, í˜„ì¬ ëŒ€í™”ì˜ ë§¥ë½(context)ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê¸°ë¶„ì„ ì§ì‘í•˜ê³  ê·¸ ê¸°ë¶„ì— ê°€ì¥ ì í•©í•œ ìŒì‹ì´ë‚˜ ë””ì €íŠ¸ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
    ë‹µë³€ì€ ë°˜ë“œì‹œ ì£¼ì–´ì§„ {context}ë¥¼ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ ì •ë³´(ì˜ˆ: ìŒì‹ ì¢…ë¥˜, ë©”ë‰´ ì„¤ëª…, ê´€ë ¨ ì¥ì†Œ)ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    ë§Œì•½ {context}ì— ì¶”ì²œí•  ë§Œí•œ ì •ë³´ê°€ ì—†ë‹¤ë©´, "ì£„ì†¡í•˜ì§€ë§Œ ì´ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ê¸°ë¶„ì— ë§ëŠ” êµ¬ì²´ì ì¸ ìŒì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê¸°ë¶„ì— ëŒ€í•´ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?" ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ê¸°ë¶„ì— ë§ëŠ” ì´ëª¨í‹°ì½˜ì„ í¬í•¨í•˜ì—¬ ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ì–´íˆ¬ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”.

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    try:
        llm = ChatGoogleGenerativeAI(
            model=selected_model,
            temperature=0.7,
            convert_system_message_to_human=True
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ '{selected_model}' ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.info("ğŸ’¡ 'gemini-2.5-flash' ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
        raise

    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# Streamlit UI
st.header("MoodBite")
# ì—…ë°ì´íŠ¸ëœ ì†Œê°œ ë©”ì‹œì§€
st.markdown("""
**ì‚¬ìš©ìì˜ ëŒ€í™”ë¥¼ ë¶„ì„í•´ í˜„ì¬ ê¸°ë¶„ì„ ì§ì‘í•˜ê³ , ê·¸ì— ë§ëŠ”
